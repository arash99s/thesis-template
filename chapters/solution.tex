\فصل{راهکار پیشنهادی}


راهکار پیشنهادی شامل ترکیب روش
\hyperref[subsubsec: ppmc]{پی.پی.اِم.سی}
 و بخشی از ایده‌ی سازوکار
\hyperref[subsubsec: ding]{دی.بیت.فلیپ.پی.اِم}
می‌باشد. این راهکار به طور همزمان دو چالش اساسی در دنیای حریم خصوصی تفاضلی محلی برای داده‌های با ابعاد بالا و در حال تغییر را هدف قرار می‌دهد. در این فصل ابتدا کلیت ساختار الگوریتم و نحوه‌ی عملکرد آن در طول زمان شرح داده‌ می‌شود. سپس جزئیات پیاده‌سازی به تفصیل بیان خواهد شد؛ در نهایت نیز راهکار پیشنهادی حاضر مورد ارزیابی قرار خواهد گرفت.

\قسمت{نوآوری پژوهش}

پژوهش حاضر با تمرکز بر جنبه‌های نوآورانه، سعی در ارائه‌ی راه‌حل‌هایی عملی و مؤثر برای حفظ حریم خصوصی داده‌های با ابعاد بالا و در حال تغییر دارد. مهم‌ترین نوآوری‌های این پژوهش عبارت‌اند از:

\شروع{شمارش}

\فقره \مهم{استفاده از سازوکار گرد کردن نقطه آلفا:} این پژوهش با به‌کارگیری تکنیک گرد کردن تصادفیِ نقطه آلفا، که در حقیقت بخشی از ایده‌ی روش دی.بیت.فلیپ.پی.اِم است، راهکاری نوین برای مدیریت چالش داده‌های در حال تغییر ارائه کرده است. برخلاف روش‌های متداول که هرگونه تغییر جزئی در داده‌های ورودی را نیازمند مصرف مجدد بودجه حریم خصوصی می‌دانند، این سازوکار با حذف نوساناتِ بی‌اهمیت و گذرا\پانویس{Jitter}، پایداری پاسخ‌های کاربران را در طول زمان تضمین می‌کند. این امر امکان استفاده مؤثر از تکنیک حفظ کردن را فراهم آورده و مانع از هدررفت سریع بودجه حریم خصوصی در جمع‌آوری‌های مکرر می‌شود.

\فقره \مهم{استفاده از روش تبدیل هار به همراه حفظ کردن:} بهره‌گیری از سازوکار 
\hyperref[subsubsec: ppmc]{تبدیل هار}
در این طرح، پاسخی راهبردی به مسئله نفرین ابعاد است. نوآوری این بخش در تلفیق این تبدیل با پروتکل‌های حریم خصوصی محلی نهفته است؛ بدین صورت که با تجزیه داده‌های چندبعدی به مؤلفه‌های میانگین و جزئیات، انرژی سیگنال در تعداد محدودی از ضرایب متمرکز می‌شود. همچنین از سازوکار حفظ کردن استفاده میکنیم تا از مصرف بیش از حد بودجه‌ی حریم خصوصی هنگام تکرار درخواست داده جلوگیری کنیم. نکته‌ی مهم اینجاست که تزریق نوفه از طریق این سازوکار، می‌تواند به راحتی روی ابعاد در حال تغییر نیز اعمال شود. از این رو لازم نیست از سازوکار تصادفی‌سازی پیچیده‌ای مثل روش دی.بیت.فلیپ.پی.اِم استفاده کرد.

\فقره \مهم{یافتن مقدار بهینه برای گامِ گسسته‌سازی:} تعیین مقدار گام گسسته‌سازی در فرآیند گرد کردن نقطه آلفا، چالش مهمی است که در این پژوهش به آن پرداخته‌ایم. برخلاف رویکردهای خلاقانه یا تجربیِ گذشته که مقدار گامِ گسسته‌سازی را ثابت فرض می‌کردند، این پژوهش یک فرمول‌بندی ریاضی دقیق برای استخراج مقدار بهینه آن ارائه می‌دهد. این نوآوری با حل مسئله‌ی کمینه‌سازی خطا، تعادلی هوشمندانه میان دقت و حریم خصوصی برقرار می‌کند. بدین ترتیب، سیستم تضمین می‌کند که سطح دانه‌بندی انتخاب شده، دقیقاً متناسب با دامنه و حساسیت داده‌ها تنظیم شود تا بیشترین مطلوبیت ممکن حاصل گردد.

\فقره \مهم{یافتن ابعاد نیازمند به گرد کردن به صورت خودکار:} این پژوهش با ارائه یک سازوکار تشخیصِ تطبیق‌پذیر\پانویس{Adaptive}، هوشمندی سیستم را در سمت کاربر ارتقا داده است. با محاسبه‌ی محلیِ «نرخ نوسانات» برای هر بُعد، این الگوریتم قادر است ابعادِ پویا و در حال تغییر را از ابعادِ ایستا تفکیک نماید. نوآوری مذکور باعث می‌شود که فرآیند گرد کردن تنها بر روی ابعادی اعمال شود که واقعاً نیازمند تثبیت هستند و از تحمیل خطای ناخواسته به ابعاد ثابت جلوگیری شود؛ رویکردی که منجر به حفظ حداکثری اطلاعات در داده‌های چندبعدی ناهمگن می‌گردد.

\پایان{شمارش}


\قسمت{بررسی چارچوب راهکار پیشنهادی}

به صورت خلاصه، با استفاده از تبدیل هار تمام داده‌ها به دو جزء میانگین و بردار ویژه تجزیه شده، در فرایند تصادفی‌سازی قرار گرفته و سمت کارپذیر ارسال می‌شوند. قبل از شروع این فرایند، باید اعمال اضافه‌تری روی داده‌های در حال تغییر انجام داد تا بتوان آنها را وارد فرایند تصادفی‌سازی کرد. شکل \رجوع{fig:thesisClient} به طور کلی رفتار الگوریتم سمت کاربر را نشان می‌دهد.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{figs/thesis-client.png}
  \caption{عملکرد روش پیشنهادی سمت کاربر}
  \label{fig:thesisClient}
\end{figure}

ابتدا ابعاد داده بر اساس ماهیت تغییراتشان دسته‌بندی می‌شوند. تمامی ابعاد لزوماً نرخ تغییر یکسانی ندارند. ابعادی که دارای تغییرات مداوم و نوسانی هستند (مانند میزان مصرف انرژی لحظه‌ای یا مختصات جغرافیایی دقیق) باید به نوعی تثبیت شوند تا تغییرات ریز باعث بروز مصرف زیاد حریم خصوصی نشود. این تثبیت باید به نوعی باشد که امید ریاضی داده‌های گرد شده برابر با داده‌های واقعی باشد و صحت آماری حفظ شود. در نهایت به تمامی ابعاد نوفه‌ای اضافه می‌شود تا حریم خصوصی آنها تضمین گردد.

\قسمت{یافتن ابعاد در حال تغییر}

در داده‌های چندبعدی واقعی، تمام ویژگی‌ها رفتار زمانی یکسانی از خود نشان نمی‌دهند. برخی از ابعاد دارای تغییرات مداوم و نوسانات سریع هستند (مانند مختصات مکانی یا مصرف لحظه‌ای منابع)، در حالی که برخی دیگر ممکن است در طول زمان ثابت مانده یا تغییرات بسیار کندی داشته باشند (مانند تنظیمات سیستمی کاربر). اعمال سازوکار گرد کردن نقطه آلفا بر روی تمامی ابعاد، رویکردی غیربهینه است؛ زیرا گرد کردن ابعادِ ثابت یا کم‌نوسان، بدون اینکه دستاورد امنیتی خاصی داشته باشد، تنها منجر به کاهش سودمندی داده‌ها می‌گردد.

برای حل این چالش، رویکرد هوشمندانه‌ای پیاده‌سازی شده است. در این روش، پیش از اعمال هرگونه پیش‌پردازش، سیستم به صورت محلی در سمت کاربر، رفتار هر بُعد را پایش کرده و تصمیم می‌گیرد که آیا آن بُعد نیازمند عبور از سازوکار گرد کردن است یا خیر. مبنای این تصمیم‌گیری، محاسبه‌ی یک معیار کمی تحت عنوان «امتیاز نوسان نرمال‌شده\پانویس{Normalized Volatility Score}» است.

برای محاسبه این امتیاز، یک پنجره زمانی لغزان به طول $W$ را برای هر بُعد $j$ در نظر می‌گیریم. میزان تغییرات مطلق داده‌ها در این پنجره محاسبه شده و نسبت به دامنه تغییرات مجاز آن بُعد نرمال‌سازی می‌شود. امتیاز نوسان $score_j$ برای بُعد $j$ از رابطه زیر به دست می‌آید:

\begin{equation}
\label{eq:score}
score_j = \frac{\sum_{t=1}^{W} |v_{j,t} - v_{j,t-1}|}{W \times (Max_j - Min_j)}
\end{equation}
در رابطه فوق، $v_{j,t}$ مقدار بُعد $j$ در لحظه $t$ است و $Max_j$ و $Min_j$ به ترتیب کران بالا و پایینِ مجاز برای آن بُعد هستند. مخرج کسر تضمین می‌کند که این معیار مستقل از مقیاس داده‌ها بوده و برای تمامی ابعاد قابل مقایسه است. مقدار $score_j$ همواره عددی بین $0$ و $1$ خواهد بود که مقادیر نزدیک به صفر نشان‌دهنده پایداری و مقادیر بزرگ‌تر نشان‌دهنده پویایی و نوسان شدید داده است.

در نهایت، تصمیم‌گیری برای فعال‌سازی مکانیزم گرد کردن بر اساس یک مقدار آستانه تجربی $\tau$ (برای مثال $0.01$) انجام می‌شود. اگر $score_j > \tau$ باشد، سیستم آن بُعد را به عنوان یک «بُعد پویا» شناسایی کرده و برای حذف نوسانات جزئی، سازوکار گرد کردن نقطه آلفا را روی آن اعمال می‌کند. در غیر این صورت، بُعد مورد نظر به عنوان «بُعد ایستا» تلقی شده و مقدار خام آن مستقیماً به مرحله بعد، یعنی تصادفی سازی با کمک تبدیل هار، ارسال می‌شود. این تفکیک هوشمندانه سبب می‌شود تا خطای ناشی از گرد کردن تنها در جایی که برای حفظ حریم خصوصی و پایداری ضروری است، هزینه شود.


\قسمت{اعمال سازوکار گرد کردن نقطه آلفا}

هسته اصلی تثبیت داده‌های در حال تغییر در این پژوهش، استفاده از روش «گرد کردن تصادفی» موسوم به نقطه آلفا است. این روش به جای گرد کردن قطعی داده‌ها به نزدیک‌ترین عدد صحیح (که منجر به خطای منطقی می‌شود)، از یک متغیر تصادفی کمکی برای تصمیم‌گیری استفاده می‌کند. این رویکرد تضمین می‌کند که اگرچه تک‌تک داده‌ها تغییر می‌کنند، اما امید ریاضی داده‌های گرد شده برابر با مقدار واقعی داده باقی می‌ماند.

فرض کنید مقدار ورودی کاربر $x$ در یک بازه مشخص با کران پایین $L$ و کران بالا $R$ قرار دارد، به طوری که طول این بازه (گام گسسته‌سازی) برابر با $s$ است ($R - L = s$). برای انجام عملیات گرد کردن، کاربر یک مقدار تصادفی $\alpha$ را از بازه $[0, s)$ انتخاب می‌کند. قاعده گرد کردن برای تولید خروجی $y$ به صورت زیر تعریف می‌شود:

\شروع{شمارش}

\فقره اگر $x + \alpha < R$ باشد، آنگاه مقدار $x$ به سمت پایین گرد شده و $y = L$ خواهد شد.

\فقره در غیر این صورت (یعنی $x + \alpha \ge R$)، مقدار $x$ به سمت بالا گرد شده و $y = R$ خواهد شد.

\پایان{شمارش}

در ادامه به تحلیل سودمندی این روش پرداخته و مواردی مانند اریب‌ناپذیری\پانویس{Unbiasedness} و خطای میانگین مربعات را بررسی می‌کنیم.

\زیرقسمت{تخمین گام گسسته‌سازی بهینه}

تعیین مقدار گام گسسته‌سازی در فرآیند گرد کردن نقطه آلفا، یک چالش بهینه‌سازی با دو هدف متضاد است. از یک سو، انتخاب مقادیر بزرگ باعث افزایش «خطای کوانتیزاسیون\پانویس{Quantization Error}» و از دست رفتن جزئیات داده می‌شود. از سوی دیگر، انتخاب مقادیر بسیار کوچک، حساسیت سیستم نسبت به نوسانات جزئی را افزایش داده و منجر به ناپایداری پاسخ‌ها و مصرف سریع بودجه حریم خصوصی می‌گردد. خطای کوانتیزاسیون یا خطای گسسته‌سازی، تفاوتِ میان «مقدار واقعی و دقیقِ ورودی» و «مقدار گرد شده‌ی خروجی» است. وقتی یک داده‌ی پیوسته را به یک داده‌ی گسسته (با دقت کمتر) تبدیل می‌کنیم، بخشی از اطلاعات دقیق آن حذف می‌شود. این مقدار حذف شده، همان خطای کوانتیزاسیون است.

پس از شناسایی ابعاد پویا، گام حیاتی بعدی تعیین مقدار بهینه‌ی گام گسسته‌سازی برای این ابعاد است. چالش اصلی در اینجا برقراری تعادل میان دو مؤلفه است: ۱) حفظ دقت داده‌ها و ۲) تضمین پایداری پاسخ‌های مربوط پرسمان‌ها.

برای استخراج یک رابطه ریاضی، ما از معیار $score_j$ بدست آمده در فرمول \رجوع{eq:score} بهره می‌بریم. هدف این است که احتمال تغییر وضعیت خروجی\پانویس{Flipping Probability} ناشی از نوسانات طبیعی داده، از یک حد آستانه مشخص ($\eta$) فراتر نرود. در سازوکار گرد کردن نقطه آلفا، احتمال $Pr_{flip}$ که نشان می‌دهد یک تغییر کوچک $\delta$ در ورودی باعث تغییر در خروجی گرد شده می‌شود، برابر است با نسبت تغییرات به اندازه گام:

$$Pr_{flip} \approx \frac{\delta}{step_j}$$

ما می‌خواهیم این احتمال کمتر یا مساوی یک حد پایداری ($\eta$) باشد (مثلاً $\eta = 0.1$ یعنی حداکثر ۱۰ درصد احتمال تغییر وضعیت). بنابراین:

$$\frac{\delta}{step_j} \le \eta \quad \Rightarrow \quad step_j \ge \frac{\delta}{\eta}$$

از طرفی، طبق تعریف ارائه‌شده در فرمول \رجوع{eq:score}، می‌دانیم که ارتباط میان تغییرات میانگین ($\delta$) و امتیاز نوسان ($score_j$) و دامنه داده ($R_j = Max_j - Min_j$) به صورت زیر است:

$$score_j = \frac{\delta}{R_j} \quad \Rightarrow \quad \delta = score_j \times R_j$$

با جایگذاری رابطه دوم در رابطه اول، به معادله زیر می‌رسیم:

$$step_j \ge \frac{score_j \times R_j}{\eta}$$

این نابرابری حداقل مقدار $step_j$ را برای تضمین پایداری مشخص می‌کند. اما برای جلوگیری از کوچک شدن بیش از حد $step_j$ در زمانی که نوسانات بسیار ناچیز است ($score_j \to 0$)، باید یک محدودیت حداقلی ($K_{base}$) نیز در نظر بگیریم. بنابراین، فرمول نهایی و جامع برای محاسبه خودکار گام گسسته‌سازی بُعد $j$ به صورت زیر پیشنهاد می‌شود:

\begin{equation}
step_j = (Max_j - Min_j) \times \max \left( \frac{1}{K_{base}} \, , \, \frac{score_j}{\eta} \right)
\end{equation}

کسر $\frac{score_j}{\eta}$ مستقیماً از رفتار داده نشأت می‌گیرد. هرچه نوسانات بُعد $j$ (یعنی $score_j$) بیشتر شود، کسر $\frac{S_j}{\eta}$ بزرگتر شده و سیستم به صورت خودکار اندازه گام ($step_j$) را افزایش می‌دهد تا نوسانات را پوشش دهد و پایداری حفظ شود.

کسر $\frac{1}{K_{base}}$ کران پایین برای اندازه گام است. متغیر $K_{base}$ (مثلاً ۱۰۰) تعیین می‌کند که حتی اگر داده کاملاً ایستا باشد، دامنه به بیش از ۱۰۰ قسمت تقسیم نشود تا سربار محاسباتی و خطای احتمالی کنترل شود.

متغیر $\eta$ در فرمول پیشنهادی، نقش یک «دریچه کنترل» را ایفا می‌کند که تعادل میان فرکانس به‌روزرسانی داده‌ها و خطای گسسته‌سازی را تنظیم می‌نماید. تعیین مقدار این متغیر وابسته به سیاست‌های کلان حفظ حریم خصوصی و محدودیت‌های سیستم است و می‌توان آن را از دو منظر تعیین نمود:

\شروع{شمارش}

\فقره رویکرد مبتنی بر سطح اطمینان: از منظر آماری، $\eta$ بیانگر حداکثر احتمال مجاز برای تغییر خروجی در اثر نوفه‌های محیطی است. اگر هدف سیستم تضمین پایداری ۹۰ درصدی برای داده‌های کاربر باشد، مقدار $\eta$ برابر با $0.1$ تنظیم می‌شود. این رویکرد در کاربردهایی که ثبات داده‌ها اولویت دارد (مانند دسته‌بندی کاربران)، مقادیر کوچکتر (مانند $0.05$) و در کاربردهایی که حساسیت به تغییرات آنی مهم است، مقادیر بزرگتر (مانند $0.2$) را می‌طلبد. در این پژوهش، بر اساس آزمایش‌های تجربی و با هدف ایجاد مصالحه میان دقت و پایداری، مقدار پیش‌فرض $\eta = 0.1$ در نظر گرفته شده است.

\فقره رویکرد مبتنی بر بودجه حریم خصوصی: در سازوکارهای حریم خصوصی تفاضلی که از تکنیک حفظ کردن استفاده می‌کنند، هر بار تغییر در مقدار خروجی، منجر به مصرف بخشی از بودجه حریم خصوصی می‌شود. با وجود محدود بودن کل بودجه حریم خصوصی باید مقدار $\eta$ را طوری تعیین کنم تا در بازه زمانی بزرگ و پایش‌های طولانی‌مدت، از اتمام سریع بودجه حریم خصوصی جلوگیری شود.

\پایان{شمارش}

\زیرقسمت{اثبات اریب‌ناپذیری}

برای اثبات اینکه این سازوکار مطلوبیت آماری داده‌ها را حفظ می‌کند، نشان می‌دهیم که امید ریاضی مقدار خروجی ($E[y]$) دقیقاً برابر با مقدار ورودی $x$ است. با توجه به توزیع یکنواخت $\alpha$، احتمال گرد کردن به سمت پایین ($P(y=L)$) و بالا ($P(y=R)$) به شرح زیر محاسبه می‌شود:

$$P(y=L) = P(x + \alpha < R) = P(\alpha < R - x) = \frac{R - x}{s}$$

$$P(y=R) = P(x + \alpha \ge R) = 1 - \frac{R - x}{s} = \frac{s - (R - x)}{s} = \frac{x - (R - s)}{s} = \frac{x - L}{s}$$

حال امید ریاضی $y$ را محاسبه می‌کنیم:

$$E[y] = L \cdot P(y=L) + R \cdot P(y=R)$$

با جایگذاری احتمالات محاسبه شده:

$$E[y] = L \cdot \left( \frac{R - x}{s} \right) + R \cdot \left( \frac{x - L}{s} \right)$$

$$E[y] = \frac{1}{s} \cdot (L \cdot R - L \cdot x + R \cdot x - R \cdot L)$$

$$E[y] = \frac{1}{s} \cdot (x \cdot (R - L))$$

از آنجا که طبق تعریف بازه، $R - L = s$ است، داریم:

$$E[y] = \frac{1}{s} \cdot (x \cdot s) = x$$

بنابراین، $E[y] = x$ اثبات می‌کند که گرد کردن نقطه آلفا یک تخمین‌گر اریب‌ناپذیر است و میانگین داده‌ها را تغییر نمی‌دهد.

\زیرقسمت{تحلیل خطای میانگین مربعات}

علاوه بر صحت میانگین، بررسی میزان پراکندگی خطا برای هر کاربر منفرد نیز حائز اهمیت است. خطای میانگین مربعات برای یک ورودی واحد به صورت $E[(y - x)^2]$ تعریف می‌شود:

$$E[(y - x)^2] = P(y=L) \cdot (L - x)^2 + P(y=R) \cdot (R - x)^2$$

با جایگذاری احتمالات:

$$E[(y - x)^2] = \left( \frac{R - x}{s} \right) \cdot (L - x)^2 + \left( \frac{x - L}{s} \right) \cdot (R - x)^2$$

با فاکتورگیری و ساده‌سازی روابط جبری (با علم به اینکه $R = L + s$)، به رابطه نهایی زیر می‌رسیم که نشان می‌دهد خطا حاصل‌ضرب فاصله داده تا دو کران بازه است:

\begin{equation}
E[(y - x)^2] = (R - x)(x - L)
\end{equation}

این رابطه نشان می‌دهد که خطای گرد کردن زمانی که $x$ به یکی از مرزهای $L$ یا $R$ نزدیک باشد به حداقل (صفر) می‌رسد و زمانی که $x$ دقیقاً در وسط بازه باشد، بیشینه خواهد بود. این رفتار تضمین می‌کند که خطای تحمیل شده همواره کراندار و کنترل‌شده باقی می‌ماند.

\قسمت{استفاده از روش تبدیل هار به همراه حفظ کردن}

ابتدا تمام داده‌ها باید در بازه‌‎ی $[-1, 1]$ یکنواخت\پانویس{Normalize} شوند. سپس با استفاده از 
\hyperref[subsubsec: ppmc]{تبدیل هار}
، داده‌های چند بعدی به دو مؤلفه مقدار میانگین و بردار ویژه تجزیه می‌شوند. مقدار میانگین با استفاده از روش پی.دی.پی (شکل \رجوع{fig:PDP}) نوفه‌دار می‌شود. به منظور تصادفی‌سازی بردار ویژه نیز از الگوریتم جی.پی.اِم \رجوع{الگوریتم: سازوکار آشفته‌سازی بردار ویژه} استفاده ‌می‌کنیم. همچنین در هر دو تصادفی‌سازی از روش حفظ کردن بهره می‌گیریم تا هم در برابر پرسش‌های مکرر ایمن باشیم و هم سرعت عملیات را افزایش دهیم. در نهایت همگی داده‌ها برای کارپذیر ارسال می‌شوند. الگوریتم \رجوع{الگوریتم: استفاده از روش تبدیل هار به همراه حفظ کردن} این مرحله را به صورت کلی نشان می‌دهد.

\شروع{الگوریتم}{استفاده از روش تبدیل هار به همراه حفظ کردن}
\ورودی داده‌های با ابعاد بالا $A = [a_1, a_2, \ldots, a_d]$، دامنه تغییرات $domains$ و بودجه‌ی حریم خصوصی $\epsilon$
\خروجی میانگین و بردار ویژه به صورت نوفه‌دار شده

\دستور یکنواخت سازی در بازه‌ی $[-1, 1]$ : $normalized = normalize(A, domains)$

\دستور استخراج میانگین و بردار ویژه: $avg, eigenvector = HaarTransform(A)$

\If {مقدار $avg$ و $eigenvector$ حفظ شده بود}

\دستور استخراج مقادیر نوفه‌دار شده از حافظه:

\دستور  $avg' , eigenvector' = getMemoized(A)$

\Else

\دستور اضافه کردن نوفه به میانگین و حفظ کردن آن: $avg' = PDP(avg, \epsilon)$

\دستور اضافه کردن نوفه به بردار ویژه و حفظ کردن آن: $eigenvector' = GPM(eigenvector, \epsilon)$

\EndIf
\دستور ارسال $avg'$ و $eigenvector'$ سمت کارپذیر

\پایان{الگوریتم}

\قسمت{جمع‌آوری و تحلیل داده‌ها توسط کارپذیر}

کارپذیر پس از دریافت مقدار میانگین و بردار ویژه‌ی نوفه‌‌دار شده، معکوس تبدیل هار را اجرا کرده و داده‌هایی نزدیک به داده‌های اصلی را بدست می‌آورد. به منظور ارزیابی، روی داده‌های در حال تغییر تخمین شمارش صورت گرفته و روی دیگر ابعاد دو معیار احتمال توزیع داده‌ها و خطای مجذور میانگین\پانویس{Mean Square Error} اندازه‌گیری می‌شود.

به منظور تخمین شمارش، برای هر بُعد لیستی به اندازه‌ی دامنه‌ی آن بُعد ساخته می‌شود که هر خانه از آن نشان‌دهنده‌ی فرکانس آن داده روی تمام داده‌های یک بُعد است. از آنجایی که داده‌ها کاملا اعشاری هستند، روی خروجی ابعاد در حال تغییر، باید ابتدا به نزدیک‌ترین مقدار صحیح گرد شوند تا بتوان تخمین شمارش را انجام داد. گفتیم که داده‌ها بر اساس متغیری به نام گام گسسته‌سازی به کران‌های بالا و پایین گرد می‌شوند. بنابراین الزاما مقدار خانه‌هایی که بین دو کران قرار می‌گیرند، صفر می‌شوند.

لیست‌های فرکانسی برای تمام ابعاد محاسبه شده و با کنار هم قرار دادن این لیست‌ها، یک ماتریس بدست می‌آید. دو بار این ماتریس محاسبه می‌شود. یکبار با داده‌های اصلی و بار دیگر با داده‌های نوفه‌دار شده. در نهایت با وارد کردن این دو ماتریس در تابع خطای مجذور میانگین، فرکانس الگوریتم خود را ارزیابی می‌کنیم.

\قسمت{یکنواخت‌سازی}

روش پی.پی.اِم.سی از تمام ابعاد میانگین گرفته و سمت کارپذیر می‌فرستد. در میانگین‌گیری اگر مقدار یک بُعد با اختلاف زیادی بیشتر از دیگر ابعاد باشد، نتیجه به سمت آن ویژگی سو می‌گیرد. پس قبل از تبدیل هار تمام ابعاد باید در یک بازه‌ی مشخص قرار گیرند. در روش پیشنهادی مانند الگوریتم پی.پی.اِم.سی تمام ابعاد در بازه‌ی $[-1, 1]$ یکنواخت می‌شوند.

این عملیات طبق الگوریتم \رجوع{الگوریتم: یکنواخت‌سازی بر اساس مقیاس‌بندی کمینه-بیشینه} با استفاده از روش مقیاس‌بندی کمینه-بیشینه\پانویس{Min-Max Normalization} پیاده‌سازی شده است.

\شروع{الگوریتم}{یکنواخت‌سازی بر اساس مقیاس‌بندی کمینه-بیشینه}
\ورودی عدد $x$ و دامنه تغییرات $domain$
\خروجی یکنواخت‌شده‌ی $x$ در بازه $[-1, 1]$

\دستور قرار بده $a = max(domain)$

\دستور قرار بده $b = min(domain)$

\دستور مقدار $((2*(x-b)) / (a-b)) - 1$ را بازگردان.

\پایان{الگوریتم}



\قسمت{تضمین حریم خصوصی تفاضلی}

در این بخش به اثبات ریاضی امن بودن روش پیشنهادی می‌پردازیم. روش پیشنهادی از راهکار تبدیل هار به منظور نوفه‌دار کردن داده‌ها استفاده شده است. در راهکار تبدیل هار، از دو سازوکار آشفته‌سازی پی.دی.پی و جی.پی.اِم استفاده می‌شود. در ادامه اثبات امن بودن این دو سازوکار بیان می‌شود.

\زیرقسمت{اثبات امن بودن سازوکار جی.پی.اِم}

سازوکار آشفته‌سازی سراسری برای حفاظت از حریم خصوصی بردار ویژه طراحی شده است. در این سازوکار، مجموعه‌ای شامل بردارهای دودویی به صورت تصادفی و مستقل از ورودی ساخته می‌شود. سپس به دو مجموعه‌ی $A$ و $B$ افراز خواهد شد. مجموعه‌ی $A$ شامل تمام بردارهایی است که تعداد اعضای 1 در آن‌ها زوج است. همچنین مجموعه‌ی $B$ شامل تمام بردارهایی است که تعداد اعضای 1 در آن‌ها فرد است. بر اساس قضیه‌ی دوجمله‌ای اندازه‌ی این دو مجموعه کاملا برابر خواهد بود. سپس یک متغیر تصادفی $X$ تعریف می‌شود که با احتمالی وابسته به $\epsilon$، یکی از دو مجموعه $A$ یا $B$ را انتخاب کرده و یک بردار به صورت تصادفی از مجموعه‌ی انتخاب شده استخراج می‌شود. در نهایت بر اساس بردار استخراج شده، بردار ویژه نوفه‌دار می‌شود.
 
برای اثبات باید نسبت احتمال تولید خروجی یکسان برای ورودی‌های متفاوت را بدست آوریم و نشان دهیم که این عدد کوچک‌تر یا مساوی $e ^ \epsilon$ خواهد بود. اکنون می‌خواهیم نسبت احتمال را برای یک خروجی دلخواه $y$ بررسی کنیم.

$$\Pr[X = 1] = \frac{e^{\varepsilon}}{e^{\varepsilon} + 1}
\quad
\Pr[X = 0] = \frac{1}{e^{\varepsilon} + 1}$$
$$
\Pr[y = v \mid V] =
\begin{cases}
\dfrac{e^{\varepsilon}}{e^{\varepsilon}+1} \cdot \dfrac{1}{|A|}, & v \in A, \\[10pt]
\dfrac{1}{e^{\varepsilon}+1} \cdot \dfrac{1}{|B|}, & v \in B .
\end{cases}$$

نسبت خروجی در بدترین حالت برای دو ورودی از دو مجموعه‌ی مختلف به صورت زیر بدست‌ می‌آید.
$$
\text{since } \hspace{5pt} |A| = |B|, \quad
\frac{\dfrac{e^{\varepsilon}}{e^{\varepsilon}+1}\cdot \dfrac{1}{|A|}}
     {\dfrac{1}{e^{\varepsilon}+1}\cdot \dfrac{1}{|B|}}
\;\;=\;\;
\frac{\dfrac{e^{\varepsilon}}{e^{\varepsilon}+1}}
     {\dfrac{1}{e^{\varepsilon}+1}}
\;\;=\;\; e^{\varepsilon}
$$

عبارت بالا نشان می‌دهد که مقدار بدست آمده همیشه کمتر از $e ^ \epsilon$ است. بنابراین سازوکار جی.پی.اِم به دلیل ساختار احتمالی و مستقل از ورودی خود، حریم خصوصی را تضمین می‌کند.

\زیرقسمت{اثبات امن بودن سازوکار پی.دی.پی}

سازوکار تصادفی یک مقدار آشفته‌شده را از یک توزیع احتمالی تولید می‌کند که شکل آن به ورودی الگوریتم بستگی دارد. توزیع احتمال به این صورت است که در ناحیه‌ی نزدیک به مقدار ورودی، احتمال انتخاب خروجی بیشتر از نواحی دیگر خواهد بود. برای اثبات $\epsilon{-}LDP$ بودن الگوریتم، باید نسبت تولید یک خروجی یکسان برای دو ورودی دلخواه را محاسبه کرده و بیشترین مقدار ممکن این نسبت را پیدا کنیم. بیشترین مقدار زمانی حاصل می‌شود که خروجی داخل بازه‌ی نزدیک به ورودی اول بوده و همچنین خارج از بازه‌ی نزدیک به ورودی دوم باشد: 


$$\frac{Pr[M(m_1) = y]}{Pr[M(m_2) = y]} = \frac{q.e^\epsilon}{q} = e^\epsilon$$


\قسمت{جمع‌بندی}

در این فصل نشان دادیم که سازوکارهای مذکور همگی حریم خصوصی تفاضلی محلی را ارضا می‌کنند. روی تمام داده‌ها (چه پویا و چه غیر پویا) سازوکارهای جی.پی.اِم و پی.دی.پی اجرا می‌شوند. پس با توجه به امن بودن چنین سازوکارهایی، می‌توان گفت حریم خصوصی به طور کامل تضمین می‌شود. از طرفی روی داده‌های در حال تغییر اعمال اضافه‌تری نیز انجام شده تا احتمال عدم قطعیت در خروجی نتایج آماری بالا رود. بنابراین می‌توان نتیجه گرفت راهکار پیشنهادی با موفقیت حریم خصوصی تفاضلی محلی را ارضا می‌کند. همچنین راهکار پیشنهادی طوری طراحی شده است تا علاوه بر حفظ حریم خصوصی، سودمندی مناسبی روی نتایج آماری داشته باشیم.







